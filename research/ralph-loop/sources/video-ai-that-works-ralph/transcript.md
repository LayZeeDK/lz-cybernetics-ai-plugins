# ðŸ¦„ ai that works: Ralph Wiggum under the hood: Coding Agent Power Tools

[https://www.youtube.com/watch?v=fOPvAPdqgPo&hl=en](https://www.youtube.com/watch?v=fOPvAPdqgPo&hl=en)

**Channel:** [Boundary](https://www.youtube.com/channel/UCt9WqM6o0y5YGqQm1nffFWQ)
**Duration:** 1:18:07

## Description

https://github.com/ai-that-works/ai-that-works

We've talked a lot about how to use context engineering to get more out of coding agents. In this week's episode, we're going to dive deep on the Ralph Wiggum Technique and why this totally different approach to coding agents can change the way you code. We'll explore using ralph for

Greenfield projects
Refactoring projects
Generating specifications

Surprise surprise, the answer is better context engineering.

ðŸ¦„ ai that works
A weekly conversation about how we can all get the most juice out of todays models with @vaibcode & @dexhorthy

---

## Transcript

is really really really fascinating in the form of almost like taking coding agents to the nth degree is how I when I first saw this concept is how I framed it in my mind is what happens when um when you just let a coding agent go wild and I think most people's perspective is nothing much or nothing nothing useful and I was convinced that that may be wrong Um, and I think that is really the topic of today's conversation for everyone interested. I'm BBOP. I work on BAML, which is a programming language for building AI stuff. And my co-host, uh, I'm Dex. I work on code layer, which is an IDE for getting more out of coding agents. >> If you guys find this kind of stuff fun, this is the link where we post events usually up ahead of time, and usually they have the right link to the live stream.

Uh, [laughter] but I think with that, we should really get started. I think we have a really special guest joining today. >> I did just send him the link. Um I told him I would do a quick intro before we got him on stage anyways. So if you want to coordinate with him on X or wherever um and make sure he can get in, I can start running through kind of what we're going to talk about today. >> Do it. Let's get people. >> Amazing. Okay, so I'm actually going to start with a demo because that seems to be pretty fun. Um, I am going to pop over to my terminal and I have a very silly little Next.js app. So, if I go to local S 3000, this is the complete generic Nex.js starter app. Um, and I have a couple things in here that I'm going to go over with you, but I just want to before I kick this off, I just want to um kind of create some specifications for an app that we want to make.

So, uh, Vibob, do you have any suggestions for an app you'd like to see us build today? >> I think we could do something really complex, but let's do something really simple so people can we can talk about the concepts more. Let's make a simple to like a task manager, like a to-do list. Very, very basic example, but we can take it to the end degree and make it as complicated as we need to. >> All right, I'm going to do login with email and magic link. Um, so what users can add to-dos, they can group to-dos into lists. They can share a list of to-dos with other users. They can add comments and emoji reactions to to-dos. Um, they can mark to-dos as to >> doing, done, cancelled. >> Yeah. while I'm at it. They can view all their to-dos for a list in a conbon board or view all to-dos across all lists in a single board.

Great. >> Uh I'm going to kick this >> simple concept and like took it to the nth degree because that is more than a basic to-do app. >> Yep. Um, so, uh, what we're going to do is we're gonna NVM deactivate and then we're going to run this this loop. Um, I'm not going to tell you what this is doing because I'm going to pop over to kind of the, um, let me just make sure we got our whiteboard up. This is not it. Okay, this is the whiteboard we'll be using. I sent you the link. Um, we're going to learn a technique today called Ralph Wigum, which was written um by Jeff Huntley as part of um, a bunch of research he was doing with coding agents this year. Um the basic idea here is you run a coding a in a loop forever. Uh and you just give it the same prompt over and over and over again. We'll go into what that prompt is.

Um and you can look at this and you can say, "Oh, this is like a very dumb way to use a coding agent." Uh and Je >> is a very dumb way to use a text. >> There he is. What's up, man? >> I remember dropping in to San Franand around about July saying just showing you this thing. It's like this has to be the dumbest thing. >> Yeah. the limits of what is possible. >> Um >> but it turns out it is >> you can make it a little bit better. There are limitations to it. Um >> yeah, there's so much. >> I think the main thing that it did for me when you first uh when I first read this concept and talked about it with both of you was really just changed the paradigm of how I thought about it. So, we talk about context injuring a lot on the show. And I think one of the things that I find myself having to do every single time and very often is challenge myself into remembering what can a model do.

Sometimes >> I'm always I'm always like I get kind of like um skill capped based on my last assumption of what the model is doing. >> And what some of the best AI engineers I know are the ones that actually like constantly challenge their own worldview of what is happening. And I think that why I like Why this topic specifically I think was really useful is because I think it was the first time when it um challenged my worldview of what a model can do and it reframed the perspective of like hey like there was a time when to build roller coaster tycoon you had to do bit magic and make all the stuff work and like today you can do it way faster and context engineering is a very very useful skill set and even today you still optimize your code but you don't optimize at the level of bit magic usually you optimize at the level of like networking different things have become bottlenecks and you want you want to make sure that you constantly evolve and Ralph Wigum as a concept of running that while loop and letting it just like produce an app I think does that same I don't think it's there yet like you were saying Jeff but I think it is >> it's not all there yet it's not all yet but but it it warps your brain a little bit um it's it's got all the I like to use the term harnesses and models so any agentic tooling out there like cursor, wind surf, all these other tools out there, open code, their harnesses, they do pretty much nothing.

They do pretty much nothing. It's all the model and how you use the model. And >> so I've got a before we get into this, Jeff, uh give everyone a quick background. Who are you? Uh what have you worked on? >> Why do you think uh you're awesome? And and let's let's quickly before you jump like tell us who you are, what you've been working on and also like let's work backwards like talk about cursed and your experience and how you learned that and then we'll pop back to the code. I have a bunch of other examples of other things you can use Ralph for that we'll get into actual like code that we'll ship as part of this episode. But yeah, give us the intro. How did you find this and what did you use it to build? Yeah. So um uh my personal journey with AI starts ran about December almost a year ago now.

Um I did a very crude running uh running an agent in a loop but I was myself attended and then through myself having to babysit this I was able to identify like some abstractions and then I learned a little bit more about what inferencing is in that sense and like context engineer in the early days even before those terms weren't even defined and I just kept playing um and I realized if I had long run one long running chat I was getting worse and worse outcomes. But if I clicked new chat and then I and I was able to encodify that up. So I was a tech lead over at Campber doing AI dev tooling. Um moved over this when I saw you uh Dex. So I was over at SourceCraft was one of the engineers building AMP. >> Um and where I am now is not public knowledge. Um typical good things. >> Yeah. Um, so, um, for me it was always >> You did something very specific that I I personally thought was very impressive.

One of your first earliest like mad crazy AI projects, cursing. >> Yeah. So, that that was like it was kind of kind of there's so many people just saying, "Hey, uh, it's not good enough yet. I'm only going to be interested when it makes a programming language." And I didn't have to pay for tokens. I didn't have to pay for tokens. I'm like, bet. >> And that's the goal, right? Is like everybody who's building with AI is like, yeah, you don't want to just spend tokens and throw them into the black hole. But the more meaning the more you can spend meaningful tokens on things that actually make progress, even if it's incremental or small progress, you want to spend as many tokens as possible. This is what we always pitch. It's a little bit of a meme, but it's like at the end of the day, that should be your goal is like how can you spend as many tokens on meaningful work with as little human input and effort as possible, right? >> Exactly.

Um I I've heard stories of companies capping uh productivity like like a max dollar spend of like $500 a month per employee type stuff. And then none of that stuff makes sense. Um you you don't want to cap uh uh people like they're when they're productive um and you do want to ensure that you you are getting incremental output. So Kirst was kind of proving that hey look it you could make a new programming language. Now, it's it's pretty much the dumbest way to do it because it'd be re much easier to clone the Golang source code and just use reax and just like relex. It >> would have been change all the keywords and leave all the source code the same. >> Just change the keywords. There's a keyword swap, right? So, it was a for me playing around with the idea of like what languages work best for LLMs.

So I started in C and I was able to see that it was uh it was able to do a prep pasa pretty well. Uh it was going quite it was making progress but it was going backwards because there was not what I described as like back pressure on the generation. So some sort of type system etc or like it lms can drive GDB and you it's able to troubleshoot itself. you add print f debug logging but it was just going backwards. So cursed originally was in C. Um then I took it over to >> curs the cursed compiler itself that like generates like >> yeah cursed compiler itself originally it's actually three compilers in the history there on GitHub. >> First first was in C. >> Go ahead. >> Um it was in C then it went to Rust then it went to Zigg. >> Let's get into it. I want to ask I want to ask a couple quick questions when you were doing this.

So let's to catch everyone else up. What is the goal of curs? So one, we wanted to make a new programming language with the goal of pushing current models to their limits and see what is possible. And I think um I can say it best, making a programming language is [Â __Â ] hard. Um I've been doing a bit you build a real programming language not as a joke. >> Um and like it is really hard. And you're right, AI models do struggle with this. And this is why I was a really big skeptic on AI coding in general for a while because like I was like there's just certain classes of problems where they don't work well in but obviously I feel differently now. Um so I want to like tell the goal of it. So cursed lang you write a cursed file. What happens from like a whiteboard? What happens next? So you write a cursed file.

The compiler is supposed to do blah fill in blah. >> Yeah sure. Um it's essentially uh it has LLVM back end um okay and it also has interpreted mode. There's a couple bugs um that I need to resolve uh to get over line. Um this thing has been the biggest test of faith for me. We'll get into that a little bit later. Um, but the ultimate goal for me was really to test the boundaries of what is possible for research and understanding how to drive an LLM really hard. For example, um there's a there's a f there's a this uh LLM um people say, well, it's not in the training data set. It can't be done. I was out there to prove that wrong. Like the fact that it can make a compiler >> not that interesting, right? There's a huge tone of knowledge. It's it fact that it can make a compiler is not that interesting to me.

What is interesting is how the heck did it generate all of the actual programs in the language when it's not in the training data set. >> So I've got I think that's a really interesting thing. We should double click on that. So the question I have for you is I think one of the decisions that you made for cursing in the very very big uh first uh is that you're not going to reinvent all the syntax. You're going to say I want to copy the syntax of go effectively and I'll change the keywords to mean something totally different where it doesn't really matter to the model and then what I will do is then I will convert that to an LVM back end. Do you leverage any of the existing Go pipelines to do any of that work or do you go straight from curs to LVM? curse straight to LLVM. >> Okay, perfect. So, that's part one.

I think just so people can understand what that is. What is LVM? A lot of people may not know. I'll just give a quick primer. LVM is the premise of like uh for those of you that don't know how to describe this in a really good concrete way. When you want to write uh at some point your code has to convert to a lot of bits and machinery that a machine operates. You don't want to run all the assembly instructions yourself. LVM is just >> you don't want to roll >> you don't want to roll for x86 you don't want to roll for ARM you don't want to run roll for different CPU architectures you just want to like a common format that you can target and that handles uh the lowering >> yeah and it also does a lot of the optimization stuff so you get to take advantage of all the optimizations and everything on there a lot of existing systems used uh LVM uh for a lot of it if you've ever used client that uses LVM I'm pretty sure JavaScript also does that but I might be wrong. >> Oh sorry that's not what I meant. >> Remember right it was an Apple invent clang came from Apple. >> Yeah. >> So this entire objective C ecosystem swift um pretty much everything targets clang.

Clang so targeting LLVM is normally the compiler's nerdy destination they want to get to. >> Yes. So the the the the point here, I guess, is the idea that like this is this is not vibe coding your Nex.js website. This is incredibly complex systems coding that is probably not as well represented in the training set. And I think what was really interesting to me about this is like Jeeoff, do you want to walk through kind of like how you've set things up? show us kind of like early version of prompt.mmdday, early versions of the specifications like what are the inputs to Ralph that that that allowed you to output a bunch of code and also like you know hundreds of markdown files, right? That's the joke that everyone talks about now is like Claude just dumping out tons of all uppercade markdown files. >> Yeah.

So it started with a simple prompt. The simple prompt is, hey, I want to make a compiler. Um, but all the lexical keywords are Gen Z. So, yeet, vibe, slay, highkey, low key, that's it. And, um, you see in tools there's a planning mode. Um, these planning modes are basically nothing. There's basically an additional prompt that uh says do not implement. So if you want to do planning, all you got to do is choose a model that's good for planning. So GPT5 is pretty good for planning. Gemini is pretty good for planning. I think I either used Sonnet or Gemini 25 in the early days and I specifically prompted do not implement your goal is to have a conversation. So the context window is best just being seen from a consumer point of view as an array that's continually appended to with messages. So I'm having a large conversation about hey I want to build a compiler.

What could a compiler need? And I'm like steering it left and right etc. And I'm not never giving it permission to implement. I'm just loading up this array with all this context. I have a discussion about different types of pasa designs. I I I talk about LVM. I going to use this language. So I'm loading up this just continually appending to this array. My both myself and also the outcomes of actually using the LLM itself. Um the result of asking a question appends to that array. Once I'm satisi satisfied, I tell it to create out a markdown markdown file called specs.md. I give it one prompt uh and that prompt is hey can you uh write out to a folder specs slash um one file per topic like for lexical one is a file grammar is a file uh yada yada yada yada and I got all these files and that's that's the baseline now um this part is this is the part that's really really important uh one one line bad co one bad line of code is one line bad line of code but like uh a bad spec will like one bad line of spec can result in tens of thousands or 50 thousands or hundreds of thousands worth of bad code output.

So when I first kicked this off, if you roll back the GitHub history to the bit the GitHub history to the beginning of time, you'll come across things like uh I had two two of the keywords for the programming language. They had a jeweled meaning cuz I the specs were wrong. So So when I kicked it off in the loop unattended, guess what it got most of the way and then it tore itself down like it was brilliant but dumb. It these things can't think. So this is um I'm just going to quickly kind of show for example some of the files that are in the specifications which is like this is actually like the grammar of the language with a bunch of examples right? >> Yeah. >> Yeah. this stuff did you read? You were running this in a pure while loop. So almost no interruptions on your end. If I was paying for tokens, I would have put a lot more care in this.

And because this was meant to be like a Dogecoin or programming languages, [laughter] um, I just kicked it off, um, and just put it on YouTube and like Twitch and just walked away. And I just watched it build and tear down build the Roman Empire and tear down the Ro Roman Empire. I think it reimplemented the Lexa uh, [laughter] probably at least 10 times. Um, and before I just cuz I'm just looking at it. Like if you look at the output of your from your harness, you can start to notice patterns and behaviors and you start asking the why is that happening? It's like, oh, I've seen it. Why is it going in a loop? Kill the the while true loop and then eventually like, oh, you idiot. So, it's really important to your point, Dex, >> is if you if you're doing this serious, >> spend your time reading the specs >> before you kick off anything. >> Yeah, >> Oh, go ahead.

One really interesting I wanted to call out was actually just the the concept here of uh that I think is really undervalued when you actually work with a coding agent versus working with a human. You said you rewrote this thing like five times. >> Yep. >> When you rewrote this thing like five times, it's fascinating because like what you can do is with a no with a person, you can't just tell someone to go delete the code and rewrite it again. There's like feelings involved. This is a motivator [laughter] feelings. >> It's it feels different to tell a person to say, "Hey, rewrite all your code versus telling an agent, I don't like that. Restart from zero." >> In some ways, that's actually a thing that people should be leveraging more and more, >> not absolutely thinking about the old way of writing code because everything in coding is almost always incremental. >> And usually the biggest tax is like, "Hey, we don't want to rip it out and redo everything again."

It's way way faster in many scenarios to actually just rip it out. You you said you redid it again from like you said see rust and sick. >> Yep. >> You could have never done that in the old world where like you need a human to go do that. You'd get so much like um like like just like stuff you have to deal with that's just like ah >> now we have to go. Turns out >> I have emotional connection with the specs >> number. When I was technically back over at Camber, I remember an engineering director >> saying, "Jeff, why did you implement it in Rust? Rust is not an approved language here." >> And I was just sitting there on the Zoom call. I just deleted the source code. Just just delete the source code. Just like, "Yep, all right. See you in 8 hours." So, I put on like Jag and this is before I was just sitting down watching like old episodes of Jag and I and I was just pressing continue continue.

It's like here it goes. It's a Golang now and it's like 40,000 lines of code in 8 hours and [laughter] it's like with full TUI this full TUI. It's the the equivalence of like the equivalence of uh Goose Goose with LCP and all this stuff just >> I want to start watching TV. >> I want to start diving a little bit deeper into some of these concepts. So you said one of the things that you were talking about earlier was like which programming language works better for AI. Let's talk about that because you said you tried three for a very complicated problem. We have chosen Rust for building a programming language and I have feelings about AI vibe coding Rust. Um, and I have feelings about AI coding C++. I have feelings about AI putting TypeScript, Python. I want to hear your perspective. What did you learn um about each of the three implementations that I think helped you understand better what the model was doing and what the model is not capable of doing?

So I'd like to uh I see the I visualize software development as bit of a wheel in the sense that the top half of the wheel is the generation phase and then that's the top half of the wheel and then eventually that wheel as it turns is going to hit the road and the road is basically where you need a bit of friction to be able to move the car forward and I call that back pressure. So with C there were there's no friction. There's no back pressure like it's an untyped language etc. And there's no back pressure. >> Back pressure. I just want to drill in. That's that's things like the model being able to run a unit test suite and understand if things are working. the model being able I mean part of the reason I think that makes programming languages such a good like example use case for unattended coding agents is precisely because you can create this loop of like it's not a website where the model has to take a picture of it with playright and then decide if it looks good or not.

It's like write the source file. If the compiler can compile it and the source does what we think it should do then it works. And so the model can give itself its own feedback without having to like have opinions just by reading or reviewing the code itself. >> Is that like kind of what how how would you expand on that in terms of how you define and think about back pressure? >> So let me pull this up. >> Um we're into an area here where this is not really vibe coding. It's more like vibe engineering. Um so um this is del you're deliberately putting your engineering hat on. So you take your your specs which is like your PD what you want to generate that's the top half of the wheel. You got your technical uh pattern library on how like logging what else have you and that's like typically language agnostic typically agnostic.

Um, and then you got the bottom part, the bottom half, and that's that's where it's your job uh to put your engineering hat on, run your unit tests, run the build, program anything in there like a security scanning software um that you would normally do. But the key thing is the wheel needs to spin fast and the the bottom half of like the cargo test clippy that I've got shown here, it needs to be enough back pressure to stop the wheel from turning if the code generation step was bad. if it if it's any form of hallucination in the output. Um, and I realized that if I look at it from the right way and like I'm just sitting down watching TV, um, and I'm just doing these things manually, I could do a while true loop. I could probably just do a bash loop. And that's how it came to be. Um, there's a question in, uh, in the, uh, in chat and I'll address it, Dave.

Yes. Uh, no one as far as I'm aware of has actually successfully did it, but I I started reimplementing OS step in three easy pieces using Ralph and it got sufficiently along the way enough where I was able to boot a VM uh so a virtual box machine. was able to read the text console out. From there, I was able to basically engineer the bottom half of the loop and I got got all got all the way like in uh past bootloadader and we start getting into like next couple chapters of OS step. Um these LLMs know what a compiler is better than I did at the time and they know what an operating system is at a implemented level better than I do. So you can you can lean upon that but you still need to have your brain on to your question Bab of like different languages. It's the speed of the wheel that matters like how fast it turns. [snorts] So if we we look at Rust, Rust is a strongly typed language and the good thing about a strongly typed language is it will reject any bad like generation.

It's very particular. That's it's things like Haskell is the same like strongly typed language is really good. Um because that provides soundness in the language by default. If the language doesn't have that soundness, you need to do more engineering like you you have to wire that stuff in. Like if you on Python, you would have to configure your like your uh your your pyite like your your actual type checkers. Um same with TypeScript on the compiler yada yada yada you get the idea. Um so but Rust's compiler is slow. Uh the Rust compile compilation speed can be really really slow. So there's a tradeoff. um you want the generation speed to be sufficiently slowed down but you still want the generation speed to be fast. So if you've got a single loop running then the speed of the generation the inferencing step and the speed of the compilation is essentially your velocity and output unless you concurrency it have some form of concurrency. now. >> So, done Elixir, I've done Zigg, I've donenet, um, I've done Java, I've played with them all like almost got into like a like a tier list >> of what languages LM are good at writing. >> Yeah.

Um, so and >> finish your point and then I wanna I want to jump back into code and like whiteboarding and kind of talk a little bit more under the hood about, you know, how do context windows work and why is like Ralph such a good strategy based on the concepts of context engineering. Does that sound good? Bye Bob. You good? >> Sounds good. Um, I would like to say it's how you drive the LLM, which will segue into context engineering >> that matters. >> Um, all the sample applications in Kurs Kirst is about 15 million lines of code folks. Okay. um and all the sample applications and all the things in there that they were generated by the LLM and the LLM was not trained on that data. It was through the context engineering I was able to create the right loop where it was able to generate program for something it hadn't been programmed on. >> So you can do some really >> make one um go ahead.

Yeah, you can do some really uh you can do some really unhinged things. Um you can either work on its inbuilt training data set, but if it lacks the training data set, it's how you do the context engine that matters. >> So let's take a let's take a deeper dive into that really fast. But before we go into that, you mentioned a couple things about the tra the loop being the most important part. I think it's really when I think about it, I think it's about trade-offs that you want to make in your engineering cycle. So, let's talk about the different trade-offs we might want to make. One is about speed. >> Guys, guys, guys, guys, guys, let's let's get practical. Sorry, I'm gonna I'm gonna I'm gonna hijack this conversation. We can keep talking about theory, but let's do a little bit of practice and we can work that use that as a as a platform to do the theory on. >> Let's show the platform.

Let's show the prompt. MD that you have. I think that's actually and then I'm going to I'm going to whiteboard out kind of how we how we like to talk about this. Um, so we have prompt MD. Um, it's a very simple prompt. This is very similar to the one that I think was the initial one that Jeff published with the Ralph paper. Jeff, you can read this prompt and romp. You can you can roast this prompt, but >> it's essentially very very short. It's read the specs, read the code and source, read the implementation plan, and implement the single highest priority feature using up to 50 sub agents. Now, I'm using Claude with the default, so it'll only do like five max. I haven't changed the Envar that lets you have more sub agents, but I left this in as a nod to curs. uh ensure all the tests and linting passes, then update implementation plan with your progress, and then commit.

And so we can see this has been working. We have a few commits going on. Phase one, phase two. Um I'm going to show you what we're actually running under the hood as well. Um and I'm actually going to make a quick tweak to this. So this is our this is our our loop. Um, I'm actually going to run this headless, but basically what we're doing is I'm running it with Claude instead of AMP, but we're running Claude with dangerously skip permissions and stream the JSON, and we're piping into this little visualizer. When you run Claude in headless mode, um, you basically get this like JSON output. Um, and so it's going to stream out all this stuff. We wrote a small little visualizer that basically makes this a little bit nicer to look at. Um, but it's the same. It's just pars. It's all it's doing is parsing that JSON.

So, we're catting that prompt over and over again. I'm going to show you when I started this um it basically um had no implementation plan file. So, it just created this based on the specs and it's been going through and creating all the CRUD operations. It's created the allegedly like the database schema. I believe it shows like um Prisma and Postgress as the but I I gave it no guidance on how to do this stuff. Um, and it's just taking notes and keeping its own like list of tasks in the plan. >> Um, and we can go and have a look and see how it's doing. I actually I saw that in the M.LE it had created uh Gmail keys. Um, which I've actually I added one more piece. This is an idea of like when might you want to actually steer this is I wanted it to use resend because I don't want to give it my Gmail keys live on the stream.

So, I just added a new feature and an example should have uh should have a resend API key. So, it has it hasn't picked this up yet. Um, but I I want to get into kind of like what's the structure of this prompt and like why is this like, you know, uh Malik and Code and compiling and all this stuff. Um, Jeff, you're a Kubernetes guy, right? >> Yeah. Um, so one of the things that we like to think about in terms of like when I think about Ralph is as in terms of control loops, right? Yep. And so every component in a lot of like complicated systems that are able to run autonomously is they have a very focused what we call control loop where you have the desired state of the world, you have the current state of the world and then you take one action to progress the current state towards the desired state and then you read it again.

And you kind of do this in a loop forever. And if all the components have the right like federated responsibilities, then this becomes a really nice model like mental model for thinking about your code. Yes. >> Yep. >> Yeah. Yeah. So, but that's one of the harder things is because everything is so undeterministic. That's why I use the image of Ralph in the oven. um that loop is either underbaked, perfectly baked or overbaked with later tendencies or things you never specified, >> right? If these two things are equal, well, the models want to be helpful. And so even if they're equal, the model will find something to do because you've invoked you've invoked the cloud CLI. It's like, well, it's it's default. It's heavily trained to do something, right? And so then it will come up with more actions and it usually defaults to changing the desired state of the world to justify its continued existence and working.

Right? >> A concrete example is when I was running a loop, it decided to add some types to the programming language. I never specified some types, but you do a 100 loops and it says it's done, it's done, it's done. on the 105th time and five times complaining it's done it hallucinates and says well a programming language should use some types and next thing you know that the way I was driving it was just completely brainless it would decided to add some sub some types to the implementation plan >> I love it baked underbaked baked or baked with latent tendencies all these kind of properties and behaviors are controllable but um that's a little bit more abstract than we can get here. Now, >> so coming coming to the context window side of this, essentially like we have a the most common way to work with AI models is you have all your system stuff and then you have your user message.

And the idea is that this will usually get you to about you know depending on how many MCPS you have somewhere between five and 60% of your context window. But let's assume you're being responsible. This gets you to like somewhere between five and 15% of your context window used. I'd like to see that you got about 176k. Ignore the million etc. It's still all it's still like 200. >> So you take 200, you minus 16 for your harness prompt to minus 16 for your LLM system prompt. Just the rough numbers. >> Yeah. 176 usable. You add your MCPs in and that comes out of 176. So now all of a sudden you're like you've had 50k of MCPs, you're now down to like 100 120k usable. >> Well, and also if you fill like if let's say you don't have a lot of MCPS, if you like the most high performance range I found for the models is like before you hit this 30 to 40% mark and anything past there.

So, if this is all full of [Â __Â ] MCPs, then you're actually doing all of your work in the like what I call the dumb zone >> where there's so much context in terms of like here's all the things you can do that it becomes much harder to actually get it to do something valuable. >> This is conceptually universal regardless which LLM that you choose or which model you choose. This is a fundamental thing. The less you use, the better it h it gets. I when I was back in February, I was noticing uh like kind of a a degrading like a degrade the more that I used it. >> Yep. >> Um and then that was just through like kind of like jazz banding kind of with the LLM like pure observation and lots of time playing with it. >> Um nothing scientific but >> I'll toss a slightly more controversial opinion. I think that was very very true a long time ago.

I think it's becoming less true um with every new release that they put out personally because they have more they have more training data for longer context data like it's so definitely true that longer context training data is very hard to collect but now with all these coding agents running a muk they have really long continuous sequences of training data >> that that I think are making the models better. Yes, they are getting better, but I've said this every at least twice a week since April, maybe even earlier, is like they are getting better, but no one will disagree even when the models support 10 million context token windows like and actually they can do something with it. No one will deny that you get better results if you use less because the attention is spread over less noise, right? the concerns.

Don't don't just like have a the way most people use cursor is they take that window there, they continually append to it and it's like sweet, it's all working now. I'm going to make the website pink. Oh, now I'm going to do some data SQL migrations and all those unrelated things are appended in there. And >> to be clear, folks, when you >> this array that gets appended, it gets appended and sent back each time. It's stateless. like you just send it back. You send it back. >> Um, so when you have unrelated things, >> it makes >> no sense. So I'm going to find a picture. realized if I useless drove the context window with and had one item per task, good outcomes. >> Yeah. And this is kind of the idea is every time you're sending the entire thing to the LLM, the only thing that determines whether it is going to pick the right next step or the wrong next step is what was already in the context window.

And coding agents are interesting because you don't actually write every word of what's in the context window. You have to kind of guide it to I mean you could paste in the contents of all the files that matter. But the idea is it's slightly easier to be like, read these four files and then go do one thing. And then when the one thing is done, then you're sending the whole conversation of like, cool, they said read these files and then they said do this thing and now they're saying do this thing. And there's just like more for the agent to think through and decide which part of this conversation is relevant to the next edit call or the next write call or the next search call. Yes. Um, all right. I want to model this a little bit differently and then I'm going to go through a couple different use cases that I love using Ralph for.

Um, and I'd love to hear your thoughts, Jeff, on which of these you've had success. the next part of Ralph. So, we're told it to create the specifications. >> Well, so, >> uh, the next >> really really quick, I just kind of want to like model how like how Ralph works in terms of a context window. Sure. >> Is so we have our desired state of the world, which is, you know, read the specs. Um, right. Read the specs, then read the source code. That's the current state of the world. And then our implementation plan is kind of like the rolling action plan. And by because we only tell it to do one thing, we end up with we end up with using like you're more likely to stay in the smart zone of the context window versus the dumb zone. >> Correct? >> And then you bomb out. You say just do one thing and then exit.

And so rather than telling, hey Codeex, keep working and don't stop working and make sure don't stop working after nine tasks and don't stop working after 10 and trying to prompt it to be able to remember to keep working. You just say, hey, you have a really easy job. Stay in the smart zone, pop one thing off of the implementation plan list and go do that and then you're done. And then actually you can >> horrendously inefficient. >> You see the top of the line where it's decide a decided state of the world, the user message and the harness, etc. Yeah, >> what you're doing is you're you're actually malicking uh and burning a lot of tokens so it can just do that one thing. It's it's highly inefficient and that because I wasn't paying for tokens, I didn't care. Um but the idea was if I could deterministically allocate the array, >> yes, >> or the context window then and I tell it to to do one thing, I didn't care if uh like to the costs.

But the idea was allocate one thing to the array to the context window and do one thing and just burn lots of tokens. Um I could have forked or clone context windows and got snazzy and stuff, but really what I'm doing is just malicking the the array the same time every loop. >> So what's interesting here is like what we have here is desired current and then implementation plan. And so what that might let you do is actually cache if you if you if you tune it just right, you may actually be able to cache this. Now I know the claude system message actually like changes based on the get dirty state, but like you could potentially set this up so that you're constantly forking off of this set and so you're only updating the current state of the worldly implementation plan. This is where context engineering comes in.

I intentionally made a mistake in this prompt because when I at@mention this it means it gets pulled in as part of the user message which means there's no way we're going to be able to cache any of that because it's going to read the implementation plan as part of the user message and so none of the rest of this can be cached >> anyways that's it's totally a tangent but that does that match your understanding? >> Yep. Um >> um >> what we've done what we got drawn out here is we told it go immediately to the implementation plan >> um but we haven't told how the implementation plan came to be. >> So the second train of thought there was >> okay I've got my specs I've generated them in the context window of research planning and then I gave it permission to write markdown then I've thrown away that array.

The next step is I would run lots of loops. Um, I was using Gemini for its large context window when I first started, probably around about Juneish. >> And maybe it was Marchish, Junish, Marchish. And um, essentially what I said was uh, >> so you would be giving it >> Sorry. Yeah, >> study the specs to understand what needs to be implemented. Then uh what I want you to do is look at the source code directory and set the two. Tell me what's not implemented. That's the high level theory. >> Yeah, run that many times. Lots of times. Lots lots of times. and then tell it to write an implementation plan. >> So when you run it lots of times like are you running it in a single context window and it's going and doing all the reading or is it like is there what's the exhaust from each run of this loop? >> I try to blow the context window up in this stage. >> You try to have as much context as possible. load as much a big the big so this is back when Gemini was >> so you're literally doing a million and then Claude was like tiny like Claude was tiny um but the idea was >> get it to execute RIP GP lots of times and get like get like no fs no fs because versus specifications what it's doing is it's executing a tool and that tool is rip grip and rip RIP powers the source code intelligence for all the coding harnesses out there.

Like Ripcript runs the world, but the input to the function of what it searches for the search query into RipG is nondeterministic. It's generative >> generative based on what it's seen so far. >> It's generative input to a to a deterministic uh tool. So I ran it lots of times. >> So you did this all in one context window over and over and over again. Let it do something >> over and over and over again. >> Run and over. And then at the very end, you say, "Write the implementation plan." >> Pretty much like I'd either do it by hand or I'd run it in a bash loop and blow the stack. >> Okay. should someone do this? Like this Ralph Wiggum technique of where you run this in a while loop, you let it rip. pull up some of the examples that we have and I'd love to get Jeff your thoughts on this. Um, and then we'll come back at the dropped out >> next.js server. >> Uh, we're going to look at some other examples. >> Yeah.

Of what of what what Ralph can do not not just for the not not just for building a new programming language from scratch, but um >> there's forward modes, there's research modes, and there's reverse modes. So this is what I have here as a reverse mode. So this is our your job is to build the specifications for we have a process that's written in Golang and we want to rewrite it in Typescript and so I've written kind of the same style of prompt to build out specifications from so it's like read the code familiarize yourself with what's in the specs but now this is the desired state of the world and this is the current state of the world because our output is specifications. Does that sound right? >> Yep. So, I've ran Ralph in reverse >> and I've cloned things that I went I heard that like commercial open source companies like crazy stuff crazy crazy stuff just ran it in full reverse and then um due to a pecuriality in Australian copyright law because it was kind of like no effort involved.

It was computer doing it. It's actually legal. So you could take a proprietary code base, >> take it up to your white label specifications, >> throw away the tainted IP and then Ralph run Ralph forwards. >> So let me u let me try and reframe this question really fast. For example, we run everything BAML's run purely in Rust. Let's say I decided tomorrow that I want to migrate our 300,000 lines of Rust code as a compiler to Zig. >> Bro, did you did you know what I was doing on this episode? This is literally my one of my examples. >> That's so funny. But um I not know that. But I have a question. Give me your overunder on how well you think that would work in terms of the most important part being opaque reliability to a user. How how much would you trust it? Because the hardest part about the system is like >> the testing coverage is very hard at the end of a real code base.

So in this scenario, I bet we could get a prototype that gets pretty close. But how how close to current parody would we actually get? The way I look at Ralph is kind of like a a principal software engineer who wants to uh get something done without having to like sit down for endless meetings. Like the specs is your kind of the PRD, the working code prototype. Um, you can use it to get essentially an army of interns to just get a concept in your idea out really fast. Um, conceptual prototype without like enough if you have an idea that you could scaffold a team around you pretty much instantly, right? And there's no nothing precious about the source code. So that's that's the level of maturity that I I think at now. I've had successes in essentially oneshotting things. It it it you got to really think about the unit the concrete unit of domain.

So I like when I was over at source graph I there was a really good um mermaid library in Golang and it had really good property based tests and because of those really good property based tests I was able to go oh I can use those property based tests as input data to drive the new generation to reject if the new generation is bad. So it was like take this Golang like mermaid diagram drawer and make it in Typescript and then we're able to just essentially transpile and use the old test data to drive the loop or the new one >> and you didn't even have to imple you didn't even need to have to implement the testing feedback back pressure loop. You just had to have it designed. You had to know what the properties you wanted to test. This is the engineering engineering not coding. >> The the people who are really really good at this like agentic engineering they'll spend three days designing the back pressure harness.

They won't even implement it. They'll just design the answer to the question how will the model know that it's working and then they'll spend three days on that and then they'll just hand it to the model and let it cook for 48 hours and they'll come back to 50,000 lines of working code. So I have a question about this because I think this premise that you're saying is very similar to why the Rust language exists. Like Rust exists like why does code in Rust mostly just work? I mean part of it's just syntax. My personal opinion is that because like testing is one of the few things that is built into Rust as a part of the core language not as a part of like an afterthought. Like in Python you have to import piest to do it. In Typescript to import Jest like test is just a syntax. >> For someone that has never seen Rust code I'll just show you guys really fast what I mean.

And I have a the reason I'm asking this is because I actually do have a question about this which is um like >> I describe this as the soundness of the language >> like right over here like I don't know about that but like I think R is beautiful but I know many people that probably would like disagree like you basically mark a test as a test thing and now you can one you get really nice hooks to run this and this is like a CLI command run you can just write test anywhere in the file it doesn't need a file any just become a test. So if I for example let's say this is sorry this is very complicated code. I'll just let's say I wanted to write a test for this function. I could literally just write really quickly cfg test. Boom. I now have a test for this function. Uh and this will test this function for me and do something with it if I wanted to.

And the fact that I can just write a test really quickly means that whenever I write Rust code, I will basically never um how do I how do I describe it? I'll basically never ever break my code because the minute I get a bug, I just add this test into here and now my code is just magically tested from here on out. >> And the fact that the language does this makes that the loop is really good. But what the question I have for you guys is what I run into is even if I write the test in theory this Ralph Wiggum technique should work with any coding agent. It it shouldn't really require the Ralph Wigum technique. I should be able to do this in cloud code in codeex and in any scenario. What is the alpha that I'm getting specifically with the Ralph Wigum technique and what is what help me distill that into like a one-s sentence um difference between just running Claude code from both of you independently.

I'd love to hear that >> the it's distilled down to I realize the LLMs understand what a compiler is better than most university students or people who are many years past like out of university unless that's their professional job. So you can lean upon that training data set and it can make decisions statistically make decisions of what the next logical order of things that should implement. So if we're not talking about compilers quickly, I've had it run through an implementation plan for um an implementation plan and it's done logging first. Like it's got 20 items to do. does logging first and then I tell it it should have telemetry and it does the telemetry next and then it's uh uh tenant awareness because I'm doing multi-tenant applications. So next thing you know it creates the uh the tenant uh middleware and none of this the the key and it it does that and then from that point forward it it builds continually on it because it's statistically making better decisions what to what to implement than most engineers do or like even best matching what I would do.

It's it's about the probably the simplest way of looking at this is we got into our profession because we loved controlling computers. It is so good to like the control aspect. I get to control the computer. The people who really get it Dex and I'll hand over to you is I feel they've surrendered the control but not their thinking. They've they've they they've they've they've learned to understand that statistically it can create and decide what the next step is as good as what you would do at times. And at times it's most of the time it's scary how good it's able to make the the next step or decision. >> But is that different than just letting it rip in cloud code? That's I think that's what I'm trying to like from a practical standpoint. I think most people that are watching and listening what they do is they use cloud code, they use codec, they use cursor uh and like what you guys are doing is slightly different technique.

You're literally running a prompt and it's still running through cloud code but there's a slightly different harness that you're using to run cloud code is the way that I describe it. >> So what is that advantage? >> So let me yeah let me let me give my take here. Um because Jeff made some good points about like hey there's a lot in the training data set that if you just kind of like let it figure itself out you will get good answers to I think for me the challenge with and I think of like model inner harness outer harness right so you have the inner harness which is something like claude code or codeex or the CLI and then you have things like that I would describe as your outer harness and I've used two different outer harnesses quite often one of them is the research plan implement workflow that works really well for brownfield code bases and lets you get like a vertical slice of relevant context in a really old codebase so you can make a valid implementation plan and the review of a human in the loop like that's our outer harness.

The other outer harness is this thing like the biggest problem with these coding agents that requires the human to be in the loop is like people say oh cloud code it'll work for a minute and then I'll have to come give it the next thing and the next thing and the next and even if I prompted we saw some someone from the AI the works community was giving a talk um at the unconference and he basically showed us his prompt and it was like do not stop after one step do not stop after two steps do not he literally his whole prompt was literally just like trying to convince the model to keep working but the problem is >> it doesn't work the models get anxiety model, it will terminate early. >> Yeah. And so there's two there's two pieces of this like one of them is like how do you get a model to work for longer than uh it wants to is you actually have it only you tell it to work for only a short time and you do that over and over again.

And then the other side is like how do you do context engineering? How do you guarantee as much work as possible? As you say Jay like Jeff like the more context you use the worse results you'll get. How do you make sure that you're doing most of the work in the smart zone as we call it? And so how like this allows you if you can set your specs and allocate them properly of like exactly how much budget you know I mean you were the first person to explain this to me Jeff was like okay you want to budget about 7% of your context window for specs 7% for finding the state of the world you know 3% for actually reading the cont the implementation plan and then you budget your you know 10 to 15% for your like base message and MCPS and all this crap y and that way you know that like because it's not just writing the files it's like 10% 20% of your context window if that's all you have right if this is 10% for doing the work you need to be able to do the number of edits you need you need to be able to run the test you need to be able to fix the test you need to be able to update the plan with your status and you need to be able to commit like two file edits is enough that like you're going to fill up 10% of your context window with everything else and so this is the other part is like long horizon by just constantly looping it through and restarting but also better results because you're designing it to only do the smallest possible chunk of work at a time. >> Yeah, the bash loop is the most abstract thing.

I was doing this all by hand in cursor back like >> check off the box >> just absolutely driving it like >> text >> just so people get an idea of what we're talking about the command line tool of like the what do you just cat it so people know what we mean by the bash loop >> yeah we'll have another look >> so it's actually >> there's no silver bullet here >> there's no silver bullet here. It's it's how you think and approach the problem domain. It's not a tool. It's not a workflow. Um you could the bash loop was nice because I started to see common failure domains and then I was able to update the bash loop to do a git reset hard or like reset the agent or all these other things. It allowed me to start thinking abstractly. >> So the biggest difference in the hot seat >> I'm going to try and summarize what I took away as the takeaway from this perspective.

If you go back whenever you show that text. >> Yeah. >> The biggest difference between running quad code or codeex or something else here versus running the walkwigum loop from what I see is this ability to go ahead and when I pull this up is this ability to say that for extremely extremely hard tasks what I need to worry about at all times is the model will not continue them. And I think I have personally experienced that and a lot of people have experienced that. What I do is because I'm usually working with Claude or Codeex, it hasn't really been a problem because I just let it come. But it's definitely a problem that if I go to bed and I definitely want Claw to keep continuing reaching to some good horizon by the end of the by the end, then I definitely need to provide that in some better mechanism.

So in that scenario, what I can do is I can either prop it and tell the model to keep going until it's actually done and hope that it does that or I can force it to go do this. And the only way I can really force it to do this is basically continuously run the process. >> Like I deterministic again, Claude. >> Yeah. >> Like basically add determinism to running forever. >> Like for and what I >> now when I run into this, can you go back? >> Uh yeah. Yeah. And then I'll I have a picture. Yeah. >> Yeah. And then when you run into this, the fundamental tradeoff that you end up here is some tasks do indeed need long context horizon. And in those tasks, this loop will not work well because if you need a long context horizon like you're you've design the system and I'm not saying there's you can't modify this loop to make it run better for those tasks.

But as stated here, this task will probably suffer because the implementation plan just may not contain enough details. And >> this is when I'd see it >> to be able to do something. >> When I see it like hill climbing towards a destination, it's just not getting there. Like it's keep resetting hard too many times. >> Um this is where you know I do haven't got here yet. I would throw away the implementation plan continually because it was so easy to regenerate the implement and you start again. Then you just kick it off and you go down to the pub and drink a beer. >> Exactly. So, it's not saying that it doesn't work. It's just that you interrupt it. You interact with it in a different way. >> like It's not like >> It's kind of like a If you're a really good M2, like a manager 2, that's kind of what you do.

You're not really going to go and interrupt. You don't want stand up bigs from your teams, but you do need a way to stop a team from going down the totally wrong direction if they've been at it for a while. >> I would. And I think this is kind of the The reason I had it streaming, V, is so I I could just rewind to a point in time and skip through and I was monitoring it as if I was an engineering manager getting my daily status updates or my Jurro board. Like it was a second monitor that was available on my phone. I was using YouTube to stream to my phone with VCR record. And I was going through reviewing all that work and the amount of times it's failed >> and it's failed because of uh the idiot in front of the computer, me, I gave it the wrong input data, the wrong implementation plan, the wrong specs or I underspecified what the objective is and it gets that behavior exactly you described. >> We've got one really important question that keeps coming up.

How does it stop? >> Uh and I know we talked about this briefly. The answer is it stops in general. You've deterministically said it should not stop. It stops when you hit control C. That is it. >> This is why you do a commit after every loop when you get a success a successful compilation. So >> it's like model training, right? It's like you overtrain the model and then you re you roll back to the actual good version, right? >> Yeah. The other part that we didn't get into was uh a perspective of how when in that loop is h how you drove the context window, how you drive the context window. I realized you could register uh new context windows in as tools in an existing context window. We call them sub aents. now. And what that meant was I could spawn a green thread like a new process green thread and anything that was a minty allocation like cargo test and maybe that that maybe that uses 200k of tokens.

I don't want 200k of tokens appended to the main inferencing loop. I want garbage collection. I went green threads with garbage collection. I went from mal malicking manually with C memory with C all the way into the JVM era. I wanted automatic garbage collection. That's the way I was looking at it. I wanted I was running a operating system process as a single process. Everyone's talking about agent agent like microservices type stuff. Um I I don't think it's a thing right now. But what what I was focusing on is keeping it simple. And the idea was how do I keep one process running forever? That loop running forever. How do I keep it running forever? And the key was don't allocate to it. And the way to not allocate to it is to spawn a green thread, >> JVM like vinking and garbage collect. So the idea is all you care about is the Unix return code on cargo test >> will and >> we're running a little bit long today. >> You summarize back to make decisions. >> I'm going to show one more thing on the how does it exit and then I'm going to hand it to you viob. >> Yeah, >> we did this >> uh we did this in a hackathon and we used it to port a bunch of repos from Python to Typescript and things like this.

Uh, one of the agents figured out it was running in an infinite loop and it figured out how to send a kill signal to its own process and stop itself. It like freaked out an existential crisis and like offed itself. But that's that's one option of how it finishes is if if the prompt is in a certain way, it will figure that out. >> Um, I have a couple things. We started a demo at the beginning of this call where we were like, we ran the raw loop for the to-do loop. Let's see it. We built a to-do app. Do you can you share a quick description of what the initial to-do app was speced out to be >> and then I want to show off what we have now and we'll start with purely the demo. So yeah, so here's the spec. You know, to-dos application login with email and magic link. Users can add to-dos. They can group to-dos into lists.

Um they can share a list of to-dos with other users. They can add comments and emoji reactions. They can mark. And then I added in the middle of this I added a one new instruction. So let's go let's go have a look at it. Um >> and it's an X.js app. So we just go look at it. Cool. Magic links. >> Yep. there. I'm going to shut resend API key manually while you >> I set that in the environment. >> Agent did not do that. Yes. >> No. Yeah, I didn't want to wait for it to do that. >> Totally valid. >> Um let's see. Post offline. >> Let's see what the app did. >> So, I'm trying >> while we're running this demo. people definitely uh we'll take questions now for the next five or 10 minutes if people have them and then we'll go um we'll try and like just like I said show the real code of what ended up concluding at the end of this content >> cuz like while we were talking a lot about this I think the best thing about this episode is we actually don't there's not much to code and show like we really just run a while loop with a very simple prompt in cloud code and let it rip and I think a lot of this why we talk so much about theory is because like >> honestly that's what's useful in like understanding why this stuff works because we'll run this code and we'll see what happens. >> So, I didn't get the email yet.

Um, so I'm just going to grab happened to log out the magic link. Yeah. So this is what exactly why we spend so much time in theory because it's not I see so many pro people going look at my perfect prompt the like buy my course of the perfect prompt or the perfect sub aent persona and all the rest like it it it it's all about like engineering is first principles thinking here um and I'm looking for someone to come up with something better than a while true bash loop. Whatever comes next is going to be really great. Like how do you solve the halting problem? Like how do you like decide when it's going to terminate? I've seen self- termination. Dex has seen self- termination. I've seen underbaking. I've seen perfectly baked and you just do get reset. This is where you put your judgment in.

And I've seen overbaked when it's it's added features because I was silly and I didn't look at the specifications or was hallucinated and just on the implementation plan decide to add something more. >> Yeah. Like running like >> think about it yourself as like an engineer. If someone gives you a checklist of all the features you want running and then they're all checked off and they're like they just give it to you of like build stuff, you will build stuff because that is what >> it just builds stuff. It it's it's kind of like the Fantasia mop, >> like the scary uh pdoom a fantasia mop when I was first coming across it cuz I realized >> it just keeps working. >> This is enough. >> It just keeps working. This is enough to to get software I don't know 70% done, 80% done. like the prototype that normally got off like all this offshoring that people normally did for 6 months or two years and they they dropped 100 grand for like a a little pock like I just don't see the place in the market for it anymore because a competent engineer can just run the loop and and they get the results back in in in a couple days. >> I I will tell you one counter-argument for why I disagree with that though.

Honestly, when I want to when I hire someone or when I when I ask some software to solve my problem, I often most of the time just want the problem solved for me. And even if I could do it myself, it's just not as interesting as just having the problem be solved for me. >> Yeah. >> And I think so like even if even if it's possible for me to go technically write the code to go do it, it's still highly useful for me to hire someone else that can just go take care of that problem and spend their time thinking about it. Because like for example right now Dexter is working on communicating with the model to make this work. Could I be [laughter] doing that 100%. Like I could go do that but it's definitely useful for someone else to be doing that and me not spending my time on there. So like when people talk about these incredibly small teams doing everything.

I think one person can definitely do more than they used to be able to do. But definitely five people all doing more than what one person individually used to do will do more than what five people used to do before as well. And some problems, you're right, don't need five people of work. But some problem, most problems that are interesting, I think do need usually at least two people's worth of work. >> There's building upon that, there's there's failure domains, liability, and warranty domains of business. Like I I I cloned um uh San Fran um uh PKI startup um just just run Ralph in reverse to specifications and ran it forward. see if it could >> Did you pull their docs? >> I put so some of the source code wasn't available cuz it's enterprise. >> Sure. documentation as additional context to reverse engineer to get the specifications cuz the a lot of the source code wasn't available and it was able to generate it really well.

But let's be clear, we're talking about PKI here, folks. Like that's a perfect example. Like just because you can do it doesn't mean you're going to do it. but you're not going to run your business on it. Um, you it's nice knowing that there's a a company that I can pay money to and they're specialized specialist in this domain. Uh Dave, your question running in reverse is literally you take an existing piece of intellectual property, whether that be proprietary software or proprietary documentation and you essentially do a clean room. You take it up to specific, you take the proprietary code, you create uh specifications >> in autonomous way >> and then you throw away the tainted intellectual property. >> Um, you need to speak with lawyers because it could be argued under US law that it's not a true clean room. >> Um, I've >> had some discussions and then you still drive forwards to implement it.

Both of you have tried this for quite a few different programs. And before we close this out, I have a couple questions that I want to ask you guys. >> One, how many of your programs got to past 80% correctness? Like are actually good and like reliable when you did this loop? >> I think it's almost always you can get to 90 or 95%. There's almost always a little bit of steering and polishing. Yeah. Jeff, what do you think? Uh it I I think software outsourcing is cooked. Um like >> you can get into the high 70s 80s. Like there's going to be holes. Um you don't trust the output. When it says it's done, it's not really done. Like reviewing code is one of the hardest things now. um because the velocity is so fast, it's it comes at you faster than you can mentally adjust and then you put a team factor into it.

It's really weird, folks. But um if I needed to generatively figure out the shape of it, like why wouldn't you not take a spec and then implement it in Zigg and then implement it in Rust and implement it in Python and then like go down to the pub, go for lunch, come back and go, this is what it could kind of look like. like really lean into the generative aspects um which allows you to capitalize on things that would there used to be a higher level emotional connection to me with code. >> It it doesn't exist anymore. Code is disposable to me now. Ideas are not. >> And the other thing is um it just feels like code's more disposable now. It's more throwaway. It's more like it's more about um it's idea to execution immediately. Idea execution, execution, execution, execution. It's all about the idea.

And that's what's changed. Like it's just this huge dopamine hit of getting outcomes. It's crazy. But it's nowhere I wouldn't call it engineering. Engineering has li professional liability to it. If you build a bridge and the build bridge collapses, then you should lose your license, should be sued. Um kind of out of time, but I think software engineering should have professional liability to it. Um just because AI generated it does not mean >> that I mean >> that like your brain you need to take accountability for it. >> The model will generate 70% done really fast, really well in a generative way. >> You can't hang up a shelf and then blame the drill. Like it's your job to make sure it's safe. is um cool. I think with that we should uh I think this episode has been really fun. Uh I think that's if Have we got the website working?

Otherwise, we'll post it later if it works. >> I'll push all the code when it's done. It's it's still churning. I think an hour to bake is not enough time for a app with that many features. And I was using Opus. I think we used it would have gone a little bit faster. I want to share one more thing that I really love. Uh thank you so much, Jeff. Uh I'm going to share my screen one more time. It's a little give give us all a little pat on the back here. Um, let's have a look and we'll we'll end on this one. Uh, this is the most interesting show on YouTube. Thank you, Jeff, for joining us and making it even more interesting than it is in in general. This was a blast. Uh, [laughter] >> until we next catch up in San Fran. >> Um, we'll get there. And then funny enough, uh Dash and I for the first time are actually a room apart uh while we're doing this thing.

We just sadly didn't have a webcam set up to go do it on the same computer. Um but for everyone else that's tuning in, uh this AI that works. We try and show real working pipelines. Um we will push the code and all the markdown files and everything that Dexter is doing here in the repo. So you'll get it as per usual. Uh next week's episode is going to go back to some interesting context endurance stuff and how to build a stick pipelines. I think that'll be interesting and fun. Uh, come join, sign up, and then we'll see you guys soon. >> Bye everyone. >> Thanks everybody.
